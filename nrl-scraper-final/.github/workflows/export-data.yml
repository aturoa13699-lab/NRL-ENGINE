name: export-data
on:
  schedule: [{ cron: "15 22 * * 2" }]   # Tue 09:15 AEST
  workflow_dispatch:
  repository_dispatch:
    types: [railway-scrape-finished]

permissions:
  contents: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  export:
    # Run schedules only on default branch
    if: ${{ github.event_name != 'schedule' || github.ref == format('refs/heads/{0}', github.event.repository.default_branch) }}
    runs-on: ubuntu-latest
    timeout-minutes: 60
    env:
      DATABASE_URL: ${{ secrets.RAILWAY_DB_URL_RO }}
      DATA_BRANCH: data
      OUT_DIR: data/exports
    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - uses: actions/setup-python@v5
        with: { python-version: "3.11", cache: "pip" }

      - name: Install deps
        run: |
          pipx install uv
          uv pip install --system "pandas>=2" "pyarrow" "fastparquet" "psycopg[binary]>=3.2"

      - name: Export matches
        id: export
        run: |
          python - <<'PY'
          import os, sys, pandas as pd
          try:
              import psycopg
          except ImportError:
              print("psycopg not available")
              print("exported=false", file=open(os.environ["GITHUB_OUTPUT"], "a"))
              sys.exit(0)

          url = os.environ.get("DATABASE_URL", "")
          out = os.environ.get("OUT_DIR", "data/exports")
          os.makedirs(out, exist_ok=True)

          if not url:
              print("DATABASE_URL not set")
              print("exported=false", file=open(os.environ["GITHUB_OUTPUT"], "a"))
              sys.exit(0)

          try:
              with psycopg.connect(url) as con:
                  df = pd.read_sql("select * from matches order by date;", con)
          except Exception as e:
              print(f"Database connection failed: {e}")
              print("exported=false", file=open(os.environ["GITHUB_OUTPUT"], "a"))
              sys.exit(0)

          if len(df) == 0:
              print("No rows in database")
              print("exported=false", file=open(os.environ["GITHUB_OUTPUT"], "a"))
              sys.exit(0)

          df.to_parquet(f"{out}/matches.parquet", index=False)
          df.to_csv(f"{out}/matches.csv", index=False)
          print(f"exported rows: {len(df)}")
          print("exported=true", file=open(os.environ["GITHUB_OUTPUT"], "a"))
          PY

      - name: Emit train_super_enriched_v2.csv
        if: steps.export.outputs.exported == 'true'
        run: |
          python - <<'PY'
          import os, pandas as pd, numpy as np
          from pathlib import Path
          out = Path(os.environ.get('OUT_DIR', 'data/exports'))
          out.mkdir(parents=True, exist_ok=True)
          mp = out / 'matches.parquet'
          df = pd.read_parquet(mp) if mp.exists() else pd.read_csv(out / 'matches.csv')
          df['date'] = pd.to_datetime(df['date'], errors='coerce')
          df['is_finals'] = df['round'].astype(str).str.lower().str.contains(
              'final|qualif|elim|semi|prelim|grand'
          ).astype(int)
          # Build team-game log for rolling stats
          h = df[['date','season','round','venue','referee','home_team','away_team',
                  'home_score','away_score','is_finals']].rename(
              columns={'home_team':'team','away_team':'opp','home_score':'pf','away_score':'pa'})
          h['is_home'] = 1
          a = df[['date','season','round','venue','referee','home_team','away_team',
                  'home_score','away_score','is_finals']].rename(
              columns={'away_team':'team','home_team':'opp','away_score':'pf','home_score':'pa'})
          a['is_home'] = 0
          g = pd.concat([h, a]).sort_values('date')
          g['win'] = (g['pf'] > g['pa']).astype(int)
          g['margin'] = (g['pf'] - g['pa']).astype(float)
          # Rolling stats (shifted to avoid leakage)
          g = g.groupby('team', group_keys=False).apply(
              lambda d: d.assign(
                  roll_win_5=d['win'].rolling(5, min_periods=1).mean().shift(1),
                  roll_margin_5=d['margin'].rolling(5, min_periods=1).mean().shift(1)
              )
          )
          H = g[g['is_home']==1][['team','date','roll_win_5','roll_margin_5']].rename(
              columns={'team':'home_team','roll_win_5':'roll_win_5_home','roll_margin_5':'roll_margin_5_home'})
          A = g[g['is_home']==0][['team','date','roll_win_5','roll_margin_5']].rename(
              columns={'team':'away_team','roll_win_5':'roll_win_5_away','roll_margin_5':'roll_margin_5_away'})
          X = df.merge(H, on=['home_team','date'], how='left').merge(A, on=['away_team','date'], how='left')
          X['home_win'] = (X['home_score'] > X['away_score']).astype(int)
          # Merge odds if available
          odds = Path('data/sources/odds.csv')
          if odds.exists():
              od = pd.read_csv(odds)
              if 'date' in od: od['date'] = pd.to_datetime(od['date'], errors='coerce')
              key = [c for c in ['date','home_team','away_team'] if c in od.columns and c in X.columns]
              if key: X = X.merge(od[key + ['home_odds_close','away_odds_close']], on=key, how='left')
          X.to_csv(out / 'train_super_enriched_v2.csv', index=False)
          print('wrote', out / 'train_super_enriched_v2.csv', 'rows', len(X))
          PY

      - name: Commit to data branch
        if: steps.export.outputs.exported == 'true'
        env:
          BR: ${{ env.DATA_BRANCH }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -e
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # Always start from latest remote state if it exists
          git fetch origin --prune
          if git ls-remote --exit-code --heads origin "$BR" >/dev/null 2>&1; then
            git checkout -B "$BR" "origin/$BR"
          else
            git checkout -B "$BR"
          fi

          git add $OUT_DIR/*
          git commit -m "data: refresh exports $(date -u +%Y%m%dT%H%M%SZ)" || echo "nothing to commit"

          # Push with retries to handle race conditions
          n=0
          until [ $n -ge 3 ]; do
            git fetch origin "$BR" 2>/dev/null || true
            if git push --force-with-lease origin HEAD:refs/heads/$BR 2>/dev/null; then
              echo "âœ… Pushed to ${BR}"
              break
            fi
            n=$((n+1))
            delay=$((n*3))
            echo "Push race; retrying in ${delay}s..."
            sleep $delay
            # Rebase on latest remote
            git fetch origin "$BR" 2>/dev/null || true
            git rebase "origin/$BR" 2>/dev/null || git rebase --abort 2>/dev/null || true
          done
          [ $n -lt 3 ] || echo "::warning::Push to data branch failed after retries"

      - uses: actions/upload-artifact@v4
        if: steps.export.outputs.exported == 'true'
        with:
          name: data-exports
          path: data/exports/*
          if-no-files-found: ignore
          retention-days: 14

      - name: Job summary
        if: always()
        run: |
          echo "### Export Data" >> $GITHUB_STEP_SUMMARY
          echo "- Exported: \`${{ steps.export.outputs.exported || 'false' }}\`" >> $GITHUB_STEP_SUMMARY
          echo "- Output: \`$OUT_DIR\`" >> $GITHUB_STEP_SUMMARY
